<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="initial-scale=1.0, user-scalable=no, width=device-width" />
    <title>Accepted Papers - NextVid Workshop@NeurIPS 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        .page-header-simple {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4rem 2rem 3rem;
            text-align: center;
            margin-bottom: 2rem;
        }
        
        .page-header-simple h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }
        
        .page-header-simple p {
            font-size: 1.1rem;
            opacity: 0.95;
        }
        
        .back-home {
            display: inline-block;
            margin: 1rem 0;
            color: white;
            text-decoration: none;
            opacity: 0.9;
            transition: opacity 0.3s;
        }
        
        .back-home:hover {
            opacity: 1;
        }
        
        .stats-bar {
            display: flex;
            justify-content: center;
            gap: 3rem;
            margin-top: 1.5rem;
            padding-top: 1.5rem;
            border-top: 1px solid rgba(255,255,255,0.3);
        }
        
        .stat-box {
            text-align: center;
        }
        
        .stat-box .number {
            font-size: 2rem;
            font-weight: 700;
            display: block;
        }
        
        .stat-box .label {
            font-size: 0.9rem;
            opacity: 0.9;
            margin-top: 0.25rem;
        }
        
        .paper-list {
            max-width: 1100px !important;
        }
        
        .section-icon {
            display: inline-block;
            margin-right: 0.5rem;
            font-size: 2rem;
        }
    </style>
</head>
<body>
    <div class="page-header-simple">
        <a href="index.html" class="back-home">‚Üê Back to Home</a>
        <h1>Accepted Papers</h1>
        <p>NextVid Workshop @ NeurIPS 2025</p>
        <div class="stats-bar">
            <div class="stat-box">
                <span class="number">5</span>
                <span class="label">Oral Presentations</span>
            </div>
            <div class="stat-box">
                <span class="number">11</span>
                <span class="label">Poster Presentations</span>
            </div>
            <div class="stat-box">
                <span class="number">16</span>
                <span class="label">Total Papers</span>
            </div>
        </div>
    </div>
    
    <main>
        <section id="oral-papers" class="section">
            <div class="container">
                <h2><span class="section-icon">üé§</span>Oral Papers</h2>
                <ul class="paper-list">
                    <li><a href="oral_papers/2_Geometry_Forcing_Marrying_Vi.pdf" target="_blank">Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling</a></li>
                    <li><a href="oral_papers/5_MUG_V_10B_High_efficiency_Tr.pdf" target="_blank">MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models</a></li>
                    <li><a href="oral_papers/10_VideoGen_of_Thought_Step_by.pdf" target="_blank">VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention</a></li>
                    <li><a href="oral_papers/12_Reinforcement_Learning_Tuni.pdf" target="_blank">Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency</a></li>
                    <li><a href="oral_papers/16_Video_Killed_the_Energy_Bud.pdf" target="_blank">Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models</a></li>
                </ul>
            </div>
        </section>

        <section id="poster-papers" class="section bg-light">
            <div class="container">
                <h2><span class="section-icon">üìä</span>Poster Papers</h2>
                <ul class="paper-list">
                    <li><a href="poster_papers/3_VIBE_Annotation_Free_Video_t.pdf" target="_blank">VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR</a></li>
                    <li><a href="poster_papers/6_DriveGen3D_Boosting_Feed_For.pdf" target="_blank">DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion</a></li>
                    <li><a href="poster_papers/7_Seeing_Beyond_the_Scene_Anal.pdf" target="_blank">Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</a></li>
                    <li><a href="poster_papers/9_The_Unwinnable_Arms_Race_of_.pdf" target="_blank">The Unwinnable Arms Race of AI Image Detection</a></li>
                    <li><a href="poster_papers/13_ZeroTrail_Zero_Shot_Traject.pdf" target="_blank">ZeroTrail: Zero-Shot Trajectory Control Framework for Video Diffusion Models</a></li>
                    <li><a href="poster_papers/19_Interaction_Aware_Video_Nar.pdf" target="_blank">Interaction-Aware Video Narrative Generation for Short-Form Gaming Content</a></li>
                    <li><a href="poster_papers/21_Petri_Net_Structure_Driven_.pdf" target="_blank">Petri Net Structure-Driven Video Generation</a></li>
                    <li><a href="poster_papers/22_Scaling_Image_and_Video_Gen.pdf" target="_blank">Scaling Image and Video Generation via Test-Time Evolutionary Search</a></li>
                    <li><a href="poster_papers/25_Learning_Human_Perceived_Fa.pdf" target="_blank">Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs</a></li>
                    <li><a href="poster_papers/26_Confidence_Scores_for_Tempo.pdf" target="_blank">Confidence Scores for Temporal Properties over Sequences of Predictions</a></li>
                    <li><a href="poster_papers/27_Reframe_Anything_LLM_Agent_.pdf" target="_blank">Reframe Anything: LLM Agent for Open World Video Reframing</a></li>
                </ul>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Contact</h3>
                    <p>Email: what.makes.good.video@gmail.com</p>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 NextVid Workshop. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
