<!DOCTYPE html>
<!-- Based on the source code from The First Workshop on Short-Form Video Understanding: The Next Frontier in Video Intelligence (ICCV 2025) -->
<!-- Source: https://short-form-video-understanding.github.io/ -->
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta
        name="viewport"
        content="initial-scale=1.0, user-scalable=no, width=device-width"
    />
    <title>What Makes a Good Video: NextVid Workshop@NeurIPS 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    >
    <style>
        /* Ensure the map container fills its section */
        #map-container {
            width: 100%;
            height: 500px;
            margin: 0;
            padding: 0;
        }
    </style>
</head>
<body>
    <header class="header">
        <nav class="nav-container">
            <!-- <div class="header-left">
                <img src="assets/tencent.png" alt="Tencent" class="sponsors-img" style="max-width: 180px;">
            </div> -->
            <ul class="nav-links">
                <li><a href="#about">About</a></li>
                <li><a href="#topics">Topics</a></li>
                <li><a href="#speakers">Speakers</a></li>
                <li><a href="#organizers">Organizers</a></li>
                <li><a href="#schedule">Schedule</a></li>
                <li><a href="#submission">Submission</a></li>
                <li><a href="#oral-papers">Accepted Papers</a></li>
                <li><a href="#area-chairs">Program Committee</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section class="hero">
            <div class="hero-content">
                <h1>What Makes a Good Video: <br> Next Practices in Video Generation and Evaluation</h1>
                <h2>@ NeurIPS 2025 Workshop</h2>
                <p class="hero-subtitle">Exploring the challenges and opportunities in video generation and evaluation</p>
                <p class="hero-subtitle">Saturday, December 6th at 8:00 AM PST.</p>
                <p class="hero-subtitle">Upper Level Ballroom 6B, San Diego Convention Center</p>
                <div class="cta-buttons">
                    <a href="#submission" class="cta-primary">Submit Paper</a>
                    <a href="#about" class="cta-secondary">Learn More</a>
                </div>
            </div>
        </section>

        <section id="about" class="section">
            <div class="container">
                <h2>About the Workshop</h2>
                <p>This workshop aims to explore the paradigm evolution for next-generation video generation models. We focus on core topics such as video generation, video understanding, benchmarks, and application implementation. Through in-depth discussions, we hope to, on the one hand, deepen the understanding of the limitations of current video models, and on the other hand, identify more valuable exploration directions for future research and practice.​ To this end, we have specially invited senior experts from both academia and industry to serve as speakers. They will deliver theme-sharing sessions from diverse perspectives, helping participants gain a comprehensive insight into the cutting-edge developments in the field.</p>
            </div>
        </section>

<section id="topics" class="section bg-light">
    <div class="container">
        <h2>Topics of Interest</h2>
        <p>We invite submissions on the following topics but are not limited to:</p>
        <br>
        <div class="topics-grid">
            <div class="topic-card">
                <h3>Video Generation Models</h3>
                <ul>
                    <li>Prompt-driven controllable synthesis</li>
                    <li>Layout &amp; motion conditioning (spatial layouts, motion trajectories)</li>
                    <li>3D / 4D / physics-informed priors for geometric and physical realism</li>
                    <li>Narrative &amp; expressive modeling to capture story structure and affect</li>
                    <li>Cross-modal generation from audio, text, or interactive inputs</li>
                    <li>Latent space control and editing for interactive generation</li>
                </ul>                
            </div>
            <div class="topic-card">
                <h3>Benchmarks & Evaluation</h3>
                <ul>
                    <li>Long-form &amp; multi-shot datasets for task-oriented assessment</li>
                    <li>Temporal-coherence evaluation: consistency over time, scene continuity, cinematic quality</li>
                    <li>Temporal / semantic / causal metrics</li>
                    <li>Human-aligned scoring for spatiotemporal and narrative fidelity</li>
                    <li>LLM-powered automated review with interpretable feedback</li>
                    <li>Reproducible benchmarking protocols</li>
                </ul>
            </div>
            <div class="topic-card">
                <h3>Applications</h3>
                <ul>
                    <li>Feedback-in-the-loop frameworks</li>
                    <li>Media &amp; content production (film, short-form video, advertising)</li>
                    <li>Education &amp; training (instructional demos, simulation-based learning)</li>
                    <li>Immersive AR/VR dynamic scene generation</li>
                    <li>Robotics &amp; simulation synthetic video for perception and planning</li>
                    <li>Interactive entertainment and gaming narratives</li>
                </ul>
            </div>
        </div>
    </div>
</section>

        

    
        <section id="speakers" class="section">
            <div class="container">
                <h2>Speakers</h2>
                <div class="speakers-grid">
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/mingyu.jpeg" alt="Ming-Yu Liu" class="profile-img">
                        </div>
                        <h3><a href="https://mingyuliu.net/" target="_blank">Ming-Yu Liu</a></h3>
                        <p class="affiliation-speaker">NVIDIA</p>
                        <p class="title">Vice President of Research</p>
                    </div>
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/minghsuan.jpg" alt="Ming-Hsuan Yang" class="profile-img">
                        </div>
                        <h3><a href="https://faculty.ucmerced.edu/mhyang/" target="_blank">Ming-Hsuan Yang</a></h3>
                        <p class="affiliation-speaker">University of California, Merced & Google DeepMind</p>
                        <p class="title">Professor</p>
                    </div>
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/saining.png" alt="Saining Xie" class="profile-img">
                        </div>
                        <h3><a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a></h3>
                        <p class="affiliation-speaker">New York University</p>
                        <p class="title">Assistant Professor</p>
                    </div>
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/jiajun.jpg" alt="Jiajun Wu" class="profile-img">
                        </div>
                        <h3><a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a></h3>
                        <p class="affiliation-speaker">Stanford University</p>
                        <p class="title">Assistant Professor</p>
                    </div>
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/dima.jpeg" alt="Dima Damen" class="profile-img">
                        </div>
                        <h3><a href="https://dimadamen.github.io/" target="_blank">Dima Damen</a></h3>
                        <p class="affiliation-speaker">University of Bristol & Google DeepMind</p>
                        <p class="title">Professor</p>
                    </div>
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/jiangyi.jpg" alt="Yi Jiang" class="profile-img">
                        </div>
                        <h3><a href="https://enjoyyi.github.io/" target="_blank">Yi Jiang</a></h3>
                        <p class="affiliation-speaker">Bytedance</p>
                        <p class="title">Research Leader</p>
                    </div>
                    <div class="speaker-card">
                        <div class="profile-image">
                            <img src="assets/xunhuang.jpg" alt="Xun Huang" class="profile-img">
                        </div>
                        <h3><a href="https://www.xunhuang.me/" target="_blank">Xun Huang</a></h3>
                        <p class="affiliation-speaker">Stealth Startup</p>
                        <p class="title">Founder & Chief Scientist</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="organizers" class="section bg-light">
            <div class="container">
                <h2>Organizers</h2>
                <div class="organizers-grid">
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/xinting.png" alt="Xinting Hu" class="profile-img">
                        </div>
                        <h3><a href="https://joyhuyy1412.github.io/" target="_blank">Xinting Hu</a></h3>
                        <p class="affiliation">Max Planck Institute for Informatics</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/yongliang.jpg" alt="Yongliang Wu" class="profile-img">
                        </div>
                        <h3><a href="https://yongliang-wu.github.io/" target="_blank">Yongliang Wu</a></h3>
                        <p class="affiliation">Southeast University</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/anna.jpg" alt="Anna Kukleva" class="profile-img">
                        </div>
                        <h3><a href="https://annusha.github.io/" target="_blank">Anna Kukleva</a></h3>
                        <p class="affiliation">Max Planck Institute for Informatics</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/zhicai.jpg" alt="Zhicai Wang" class="profile-img">
                        </div>
                        <h3><a href="https://scholar.google.com/citations?user=21xyM8oAAAAJ" target="_blank">Zhicai Wang</a></h3>
                        <p class="affiliation">University of Science and Technology of China</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/chenyang.jpg" alt="Chenyang Si" class="profile-img">
                        </div>
                        <h3><a href="https://chenyangsi.top/" target="_blank">Chenyang Si</a></h3>
                        <p class="affiliation">Nanjing University</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/lijiang.jpg" alt="Jiang Li" class="profile-img">
                        </div>
                        <h3><a href="https://llijiang.github.io/" target="_blank">Li Jiang</a></h3>
                        <p class="affiliation">Chinese University of Hong Kong, Shenzhen</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/yugang.jpg" alt="Gang Yu" class="profile-img">
                        </div>
                        <h3><a href="https://www.skicyyu.org/" target="_blank">Gang Yu</a></h3>
                        <p class="affiliation">StepFun</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/xuyang.png" alt="Xu Yang" class="profile-img">
                        </div>
                        <h3><a href="https://yxpalmweb.github.io/" target="_blank">Xu Yang</a></h3>
                        <p class="affiliation">Southeast University</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/ziwei.jpeg" alt="Ziwei Liu" class="profile-img">
                        </div>
                        <h3><a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a></h3>
                        <p class="affiliation">Nanyang Technological University</p>
                    </div>
                    <div class="organizer-card">
                        <div class="profile-image">
                            <img src="assets/bernt.jpeg" alt="Bernt Schiele" class="profile-img">
                        </div>
                        <h3><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele" target="_blank">Bernt Schiele</a></h3>
                        <p class="affiliation">Max Planck Institute for Informatics</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="schedule" class="section">
            <div class="container">
                <h2>Workshop Schedule</h2>
                <div class="schedule-timeline">
                    <div class="timeline-item">
                        <div class="time">One-Day</div>
                        <div class="event">Schedule TBD</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="submission" class="section bg-light animate">
            <div class="container">
                <h2>Paper Submission</h2>
                <div class="submission-info">
                    <div class="submission-details">
                        <table class="important-dates-table">
                            <tbody>
                            <tr>
                                <td class="event-col"><strong>Submissions Due</strong></td>
                                <td class="date-col">August 29, 2025 AoE</td>
                            </tr>
                            <tr>
                                <td class="event-col"><strong>Author Notification</strong></td>
                                <td class="date-col">September 22, 2025 AoE</td>
                            </tr>
                            <tr>
                                <td class="event-col"><strong>Camera-Ready Due</strong></td>
                                <td class="date-col">September 29, 2025 AoE</td>
                            </tr>
                        </tbody></table>
                    </div>
                    <div class="submission-guidelines">
                        <h3>Submission Guidelines</h3>
                        <p>We welcome the following types of submissions:</p>
                        <ul class="submission-guidelines-list">
                            <li><strong>Full Paper</strong>: In this track, we welcome the submissions that are intended to demonstrate original research ideas and their impacts, and have not been published to other conferences/journals. The text length should be 5-9 pages (excluding references and appendices).</li>
                            <li><strong>Short Paper</strong>: In this track, we welcome the submissions that are intended for reporting promising early-stage research, novel ideas, or results that may not yet be fully developed for a full paper. The text length should be 2-4 pages (excluding references and appendices).</li>
                        </ul>
                        <br>
                        All submissions will be featured in the workshop poster session to give the authors the opportunity to present their work, and a subset of the submissions will be selected for a oral talk session during the workshop. All accepted papers are non-archival.
                        <br>
                        <br>
                        We employ a double-blind review process conducted through the OpenReview, and all papers must adhere to the <a href="https://neurips.cc/Conferences/2025/CallForPapers" target="_blank">NeurIPS 2025 submission format</a> and use NeurIPS 2025 latex template.
                        <br>
                        <br>
                        Submission Portal: <a href="https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/NextVid" target="_blank">OpenReview</a>
                        <br>
                        <br>
                        <h3>Review Guidelines</h3>
                        Our review process follows the standards of NeurIPS 2025, and reviewers are expected to adhere to the same <a href="https://neurips.cc/Conferences/2025/ReviewerGuidelines" target="_blank">NeurIPS 2025 reviewer guidelines</a>. We sincerely thank all reviewers for their dedication to maintaining the high academic standards of the NextVid Workshop@NeurIPS 2025!
                    </div>
                </div>
        </section>

        <section id="oral-papers" class="section">
            <div class="container">
                <h2><span class="section-icon">🎤</span>Oral Papers</h2>
                <ul class="paper-list">
                    <li><a href="assets/2_Geometry_Forcing_Marrying_Vi.pdf" target="_blank">Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling</a></li>
                    <li><a href="assets/5_MUG_V_10B_High_efficiency_Tr.pdf" target="_blank">MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models</a></li>
                    <li><a href="assets/10_VideoGen_of_Thought_Step_by.pdf" target="_blank">VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention</a></li>
                    <li><a href="assets/12_Reinforcement_Learning_Tuni.pdf" target="_blank">Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency</a></li>
                    <li><a href="assets/16_Video_Killed_the_Energy_Bud.pdf" target="_blank">Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models</a></li>
                </ul>
            </div>
        </section>

        <section id="poster-papers" class="section bg-light">
            <div class="container">
                <h2><span class="section-icon">📊</span>Poster Papers</h2>
                <ul class="paper-list">
                    <li><a href="assets/3_VIBE_Annotation_Free_Video_t.pdf" target="_blank">VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR</a></li>
                    <li><a href="assets/6_DriveGen3D_Boosting_Feed_For.pdf" target="_blank">DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion</a></li>
                    <li><a href="assets/7_Seeing_Beyond_the_Scene_Anal.pdf" target="_blank">Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</a></li>
                    <li><a href="assets/9_The_Unwinnable_Arms_Race_of_.pdf" target="_blank">The Unwinnable Arms Race of AI Image Detection</a></li>
                    <li><a href="assets/13_ZeroTrail_Zero_Shot_Traject.pdf" target="_blank">ZeroTrail: Zero-Shot Trajectory Control Framework for Video Diffusion Models</a></li>
                    <li><a href="assets/19_Interaction_Aware_Video_Nar.pdf" target="_blank">Interaction-Aware Video Narrative Generation for Short-Form Gaming Content</a></li>
                    <li><a href="assets/21_Petri_Net_Structure_Driven_.pdf" target="_blank">Petri Net Structure-Driven Video Generation</a></li>
                    <li><a href="assets/22_Scaling_Image_and_Video_Gen.pdf" target="_blank">Scaling Image and Video Generation via Test-Time Evolutionary Search</a></li>
                    <li><a href="assets/25_Learning_Human_Perceived_Fa.pdf" target="_blank">Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs</a></li>
                    <li><a href="assets/26_Confidence_Scores_for_Tempo.pdf" target="_blank">Confidence Scores for Temporal Properties over Sequences of Predictions</a></li>
                    <li><a href="assets/27_Reframe_Anything_LLM_Agent_.pdf" target="_blank">Reframe Anything: LLM Agent for Open World Video Reframing</a></li>
                </ul>
            </div>
        </section>

        <section id="area-chairs" class="section">
            <div class="container">
                <h2><span class="section-icon">👥</span>Area Chairs</h2>
                <ul class="committee-list">
                    <li><a href="https://openreview.net/profile?id=~Beier_Zhu1" target="_blank">Beier Zhu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Haoran_Wang3" target="_blank">Haoran Wang</a></li>
                    <li><a href="https://openreview.net/profile?id=~Jiahui_Liu2" target="_blank">Jiahui Liu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Yicheng_Fu1" target="_blank">Yicheng Fu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Suncheng_Xiang1" target="_blank">Suncheng Xiang</a></li>
                    <li><a href="https://openreview.net/profile?id=~Xingyu_Zhu3" target="_blank">Xingyu Zhu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Anjiang_Wei1" target="_blank">Anjiang Wei</a></li>
                    <li><a href="https://openreview.net/profile?id=~Chirui_CHANG1" target="_blank">Chirui Chang</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zhan_Tong1" target="_blank">Zhan Tong</a></li>
                    <li><a href="https://openreview.net/profile?id=~Jinqi_Luo1" target="_blank">Jinqi Luo</a></li>
                    <li><a href="https://openreview.net/profile?id=~Kaiyue_Sun1" target="_blank">Kaiyue Sun</a></li>
                    <li><a href="https://openreview.net/profile?id=~Xinyu_Ye2" target="_blank">Xinyu Ye</a></li>
                </ul>
            </div>
        </section>

        <section id="reviewers" class="section bg-light">
            <div class="container">
                <h2><span class="section-icon">📝</span>Reviewers</h2>
                <ul class="committee-list">
                    <li><a href="https://openreview.net/profile?id=~Jingze_Zhu1" target="_blank">Jingze Zhu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Mingzhe_Zheng1" target="_blank">Mingzhe Zheng</a></li>
                    <li><a href="https://openreview.net/profile?id=~Matin_Khajavi1" target="_blank">Matin Khajavi</a></li>
                    <li><a href="https://openreview.net/profile?id=~Wenyu_Zhang2" target="_blank">Wenyu Zhang</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zeyuan_Yin1" target="_blank">Zeyuan Yin</a></li>
                    <li><a href="https://openreview.net/profile?id=~Haonan_Lin1" target="_blank">Haonan Lin</a></li>
                    <li><a href="https://openreview.net/profile?id=~Xinyu_Xiong1" target="_blank">Xinyu Xiong</a></li>
                    <li><a href="https://openreview.net/profile?id=~Xuehao_Gao1" target="_blank">Xuehao Gao</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zhicheng_Zhang1" target="_blank">Zhicheng Zhang</a></li>
                    <li><a href="https://openreview.net/profile?id=~Hyoung-Kyu_Song1" target="_blank">Hyoung-Kyu Song</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zhou_Qin2" target="_blank">Zhou Qin</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zonghui_Li1" target="_blank">Zonghui Li</a></li>
                    <li><a href="https://openreview.net/profile?id=~Guodong_Xu3" target="_blank">Guodong Xu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zhiyang_Dou1" target="_blank">Zhiyang Dou</a></li>
                    <li><a href="https://openreview.net/profile?id=~Bum_Jun_Kim1" target="_blank">Bum Jun Kim</a></li>
                    <li><a href="https://openreview.net/profile?id=~Qi_Zhao6" target="_blank">Qi Zhao</a></li>
                    <li><a href="https://openreview.net/profile?id=~Dehai_Min1" target="_blank">Dehai Min</a></li>
                    <li><a href="https://openreview.net/profile?id=~YU_LI54" target="_blank">YU LI</a></li>
                    <li><a href="https://openreview.net/profile?id=~Zhihao_Hao1" target="_blank">Zhihao Hao</a></li>
                    <li><a href="https://openreview.net/profile?id=~jichao_zhang1" target="_blank">Jichao Zhang</a></li>
                    <li><a href="https://openreview.net/profile?id=~Yishan_Liu1" target="_blank">Yishan Liu</a></li>
                    <li><a href="https://openreview.net/profile?id=~kaiyuan_liu3" target="_blank">Kaiyuan Liu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Huaijin_Wu1" target="_blank">Huaijin Wu</a></li>
                    <li><a href="https://openreview.net/profile?id=~Jingjing_Chang1" target="_blank">Jingjing Chang</a></li>
                </ul>
            </div>
        </section>



    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>Contact</h3>
                    <p>Email: what.makes.good.video@gmail.com</p>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 NextVid Workshop. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
